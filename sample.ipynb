{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a77807f92f26ee",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "id": "fbc121e30a2defb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T01:23:15.595288Z",
     "start_time": "2025-03-13T01:23:14.995068Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For NLP chatbot\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\farou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\farou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\farou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "id": "6e16c82b9c82a08f",
   "metadata": {},
   "source": [
    "# Data Loading and Exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "c3c4253ba58939f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T01:23:15.647679Z",
     "start_time": "2025-03-13T01:23:15.608934Z"
    }
   },
   "source": [
    "df = pd.read_csv('data/edited_skill_exchange_dataset.csv')"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "id": "d4d3c9e161375343",
   "metadata": {},
   "source": [
    "\n",
    "# Display basic information about the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "eb947f599c010912",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T01:23:15.769332Z",
     "start_time": "2025-03-13T01:23:15.759447Z"
    }
   },
   "source": [
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nSample Data:\")\n",
    "print(df.head())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (10000, 6)\n",
      "\n",
      "Data Types:\n",
      "user_id            int64\n",
      "joinedDate        object\n",
      "joinedCourses     object\n",
      "skills            object\n",
      "desired_skills    object\n",
      "isVerified          bool\n",
      "dtype: object\n",
      "\n",
      "Sample Data:\n",
      "   user_id  joinedDate                            joinedCourses  \\\n",
      "0        1  2022-08-28  Machine Learning, CSS, Excel, SQL, HTML   \n",
      "1        2  2023-12-04  Data Science, Excel, Python, JavaScript   \n",
      "2        3  2023-04-10          JavaScript, Python, Excel, Java   \n",
      "3        4  2022-01-30              AI, Machine Learning, Excel   \n",
      "4        5  2022-09-07                                   Python   \n",
      "\n",
      "                               skills  \\\n",
      "0                           HTML, SQL   \n",
      "1   HTML, CSS, JavaScript, Excel, SQL   \n",
      "2                     HTML, CSS, Java   \n",
      "3  HTML, Excel, SQL, Java, Blockchain   \n",
      "4                 CSS, JavaScript, AI   \n",
      "\n",
      "                                      desired_skills  isVerified  \n",
      "0  CSS, Java, Machine Learning, Blockchain, Data ...       False  \n",
      "1              JavaScript, Python, Java, Node.js, AI        True  \n",
      "2                                  CSS, SQL, Node.js        True  \n",
      "3  SQL, Node.js, Machine Learning, Blockchain, Da...        True  \n",
      "4                    HTML, CSS, Java, AI, Blockchain       False  \n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "id": "e82f1d3247de28ea",
   "metadata": {},
   "source": [
    "## 2. Data Preparation and Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "577b2c28632f85ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T01:23:15.948766Z",
     "start_time": "2025-03-13T01:23:15.858854Z"
    }
   },
   "source": [
    "# Convert joinedDate to datetime\n",
    "df['joinedDate'] = pd.to_datetime(df['joinedDate'])\n",
    "\n",
    "# Calculate membership duration (in days)\n",
    "df['membershipDuration'] = (pd.Timestamp('2025-03-12') - df['joinedDate']).dt.days\n",
    "\n",
    "# Function to clean and standardize comma-separated text fields\n",
    "def clean_text_list(text):\n",
    "    if isinstance(text, str):\n",
    "        # Split by comma, strip whitespace, and rejoin\n",
    "        items = [item.strip() for item in text.split(',')]\n",
    "        return ', '.join(items)\n",
    "    return text\n",
    "\n",
    "# Apply cleaning to text columns\n",
    "for col in ['joinedCourses', 'skills', 'desired_skills']:\n",
    "    df[col] = df[col].apply(clean_text_list)\n",
    "\n",
    "# Create count features\n",
    "df['course_count'] = df['joinedCourses'].apply(lambda x: len(x.split(',')) if isinstance(x, str) else 0)\n",
    "df['skills_count'] = df['skills'].apply(lambda x: len(x.split(',')) if isinstance(x, str) else 0)\n",
    "df['desired_skills_count'] = df['desired_skills'].apply(lambda x: len(x.split(',')) if isinstance(x, str) else 0)\n",
    "\n",
    "# Display the cleaned data\n",
    "print(\"\\nCleaned Dataset:\")\n",
    "print(df.head())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned Dataset:\n",
      "   user_id joinedDate                            joinedCourses  \\\n",
      "0        1 2022-08-28  Machine Learning, CSS, Excel, SQL, HTML   \n",
      "1        2 2023-12-04  Data Science, Excel, Python, JavaScript   \n",
      "2        3 2023-04-10          JavaScript, Python, Excel, Java   \n",
      "3        4 2022-01-30              AI, Machine Learning, Excel   \n",
      "4        5 2022-09-07                                   Python   \n",
      "\n",
      "                               skills  \\\n",
      "0                           HTML, SQL   \n",
      "1   HTML, CSS, JavaScript, Excel, SQL   \n",
      "2                     HTML, CSS, Java   \n",
      "3  HTML, Excel, SQL, Java, Blockchain   \n",
      "4                 CSS, JavaScript, AI   \n",
      "\n",
      "                                      desired_skills  isVerified  \\\n",
      "0  CSS, Java, Machine Learning, Blockchain, Data ...       False   \n",
      "1              JavaScript, Python, Java, Node.js, AI        True   \n",
      "2                                  CSS, SQL, Node.js        True   \n",
      "3  SQL, Node.js, Machine Learning, Blockchain, Da...        True   \n",
      "4                    HTML, CSS, Java, AI, Blockchain       False   \n",
      "\n",
      "   membershipDuration  course_count  skills_count  desired_skills_count  \n",
      "0                 927             5             2                     5  \n",
      "1                 464             4             5                     5  \n",
      "2                 702             4             3                     3  \n",
      "3                1137             3             5                     5  \n",
      "4                 917             1             3                     5  \n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "id": "e697925a143e0712",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Data Understanding and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "id": "8a7ca75d2db4fcb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T01:23:17.372539Z",
     "start_time": "2025-03-13T01:23:16.119530Z"
    }
   },
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Distribution of course counts\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.histplot(df['course_count'], kde=True)\n",
    "plt.title('Distribution of Course Counts')\n",
    "plt.xlabel('Number of Courses')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Distribution of skill counts\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.histplot(df['skills_count'], kde=True)\n",
    "plt.title('Distribution of Skill Counts')\n",
    "plt.xlabel('Number of Skills')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Distribution of desired skill counts\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.histplot(df['desired_skills_count'], kde=True)\n",
    "plt.title('Distribution of Desired Skill Counts')\n",
    "plt.xlabel('Number of Desired Skills')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Membership duration distribution\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.histplot(df['membershipDuration'], kde=True)\n",
    "plt.title('Distribution of Membership Duration')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('distributions.png')\n",
    "plt.close()"
   ],
   "outputs": [],
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "id": "9065e128745242c7",
   "metadata": {},
   "source": [
    "# Function to extract all unique skills/courses from a column"
   ]
  },
  {
   "cell_type": "code",
   "id": "33b46cd18c959443",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T01:23:17.387349Z",
     "start_time": "2025-03-13T01:23:17.380556Z"
    }
   },
   "source": [
    "def extract_unique_items(df, column_name):\n",
    "    all_items = []\n",
    "    for items_str in df[column_name]:\n",
    "        if isinstance(items_str, str):\n",
    "            items = [item.strip() for item in items_str.split(',')]\n",
    "            all_items.extend(items)\n",
    "    return list(set(all_items))"
   ],
   "outputs": [],
   "execution_count": 47
  },
  {
   "cell_type": "markdown",
   "id": "62cb98c4bcb58b41",
   "metadata": {},
   "source": [
    "# Get unique items\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "883f21790111ea3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T01:23:17.522686Z",
     "start_time": "2025-03-13T01:23:17.470895Z"
    }
   },
   "source": [
    "all_courses = extract_unique_items(df, 'joinedCourses')\n",
    "all_skills = extract_unique_items(df, 'skills')\n",
    "all_desired_skills = extract_unique_items(df, 'desired_skills')\n",
    "print(f\"\\nTotal unique courses: {len(all_courses)}\")\n",
    "print(f\"Total unique skills: {len(all_skills)}\")\n",
    "print(f\"Total unique desired skills: {len(all_desired_skills)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total unique courses: 12\n",
      "Total unique skills: 13\n",
      "Total unique desired skills: 13\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "cell_type": "markdown",
   "id": "dc0ae70756391d2",
   "metadata": {},
   "source": [
    "# Top courses visualization"
   ]
  },
  {
   "cell_type": "code",
   "id": "7709cad99d6f128",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T01:23:18.526435Z",
     "start_time": "2025-03-13T01:23:17.680980Z"
    }
   },
   "source": [
    "def plot_top_items(df, column_name, title, n=10):\n",
    "    all_items = []\n",
    "    for items_str in df[column_name]:\n",
    "        if isinstance(items_str, str):\n",
    "            items = [item.strip() for item in items_str.split(',')]\n",
    "            all_items.extend(items)\n",
    "\n",
    "    item_counts = pd.Series(all_items).value_counts().head(n)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x=item_counts.values, y=item_counts.index)\n",
    "    plt.title(f'Top {n} {title}')\n",
    "    plt.xlabel('Count')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'top_{column_name}.png')\n",
    "    plt.close()\n",
    "\n",
    "# Plot top items for each category\n",
    "plot_top_items(df, 'joinedCourses', 'Courses')\n",
    "plot_top_items(df, 'skills', 'Skills')\n",
    "plot_top_items(df, 'desired_skills', 'Desired Skills')"
   ],
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Feature Engineering for Skill Matching\n",
   "id": "35b80afbbac0143c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T01:23:18.717287Z",
     "start_time": "2025-03-13T01:23:18.533563Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create binary features for skills and desired skills using CountVectorizer\n",
    "def create_binary_features(df, column_name):\n",
    "    vectorizer = CountVectorizer(tokenizer=lambda x: [item.strip() for item in x.split(',')])\n",
    "    binary_features = vectorizer.fit_transform(df[column_name].fillna(''))\n",
    "    binary_df = pd.DataFrame(binary_features.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    return binary_df, vectorizer\n",
    "\n",
    "# Create binary feature matrices\n",
    "skills_binary, skills_vectorizer = create_binary_features(df, 'skills')\n",
    "desired_skills_binary, desired_skills_vectorizer = create_binary_features(df, 'desired_skills')\n",
    "courses_binary, courses_vectorizer = create_binary_features(df, 'joinedCourses')\n",
    "\n",
    "# Combine features for clustering\n",
    "combined_features = pd.concat([\n",
    "    skills_binary,\n",
    "    desired_skills_binary,\n",
    "    courses_binary,\n",
    "    df[['membershipDuration', 'course_count', 'skills_count', 'desired_skills_count', 'isVerified']].reset_index(drop=True)\n",
    "], axis=1)\n"
   ],
   "id": "96efd51e8ee92d1f",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Scale numerical features",
   "id": "954abbeb10251874"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T01:23:18.840746Z",
     "start_time": "2025-03-13T01:23:18.819535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "scaler = StandardScaler()\n",
    "numerical_features = ['membershipDuration', 'course_count', 'skills_count', 'desired_skills_count']\n",
    "combined_features[numerical_features] = scaler.fit_transform(combined_features[numerical_features])\n",
    "\n",
    "# Convert boolean to int\n",
    "combined_features['isVerified'] = combined_features['isVerified'].astype(int)\n",
    "\n",
    "print(\"\\nFeature Engineering Complete\")\n",
    "print(f\"Total features: {combined_features.shape[1]}\")"
   ],
   "id": "a5d838ae3e90fe7e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Engineering Complete\n",
      "Total features: 43\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Clustering with KMeans (KMM)\n",
   "id": "f9909ddec1409373"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T01:23:20.769695Z",
     "start_time": "2025-03-13T01:23:18.924897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Determine optimal number of clusters using Elbow Method\n",
    "inertia = []\n",
    "k_range = range(2, 8)  # For a small dataset, we'll test up to 7 clusters\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(combined_features)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plot Elbow Method\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, inertia, marker='o')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.grid(True)\n",
    "plt.savefig('elbow_method.png')\n",
    "plt.close()"
   ],
   "id": "8acfb43beefbdac6",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T01:23:22.690268Z",
     "start_time": "2025-03-13T01:23:20.873584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optimal_k = 4\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(combined_features)\n",
    "\n",
    "# Add cluster labels to the original dataframe\n",
    "df['cluster'] = cluster_labels\n",
    "\n",
    "# Analyze clusters\n",
    "cluster_stats = df.groupby('cluster').agg({\n",
    "    'membershipDuration': 'mean',\n",
    "    'course_count': 'mean',\n",
    "    'skills_count': 'mean',\n",
    "    'desired_skills_count': 'mean',\n",
    "    'isVerified': 'mean',\n",
    "    'user_id': 'count'\n",
    "}).rename(columns={'user_id': 'count'})\n",
    "\n",
    "print(\"\\nCluster Statistics:\")\n",
    "print(cluster_stats)\n",
    "\n",
    "# Visualize clusters\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(\n",
    "    x='skills_count',\n",
    "    y='desired_skills_count',\n",
    "    hue='cluster',\n",
    "    size='course_count',\n",
    "    sizes=(50, 200),\n",
    "    palette='viridis',\n",
    "    data=df\n",
    ")\n",
    "plt.title('User Clusters by Skills and Desired Skills')\n",
    "plt.xlabel('Number of Current Skills')\n",
    "plt.ylabel('Number of Desired Skills')\n",
    "plt.savefig('user_clusters.png')\n",
    "plt.close()"
   ],
   "id": "e5fb9ad06ae70d72",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster Statistics:\n",
      "         membershipDuration  course_count  skills_count  desired_skills_count  \\\n",
      "cluster                                                                         \n",
      "0                799.872926      2.066775      4.916633              4.631728   \n",
      "1                799.262735      1.855228      2.215626              4.676369   \n",
      "2                800.730166      3.832565      3.327030              6.885148   \n",
      "3                795.425455      4.309818      3.277091              3.734545   \n",
      "\n",
      "         isVerified  count  \n",
      "cluster                     \n",
      "0          0.506273   2471  \n",
      "1          0.502872   2611  \n",
      "2          0.519373   2168  \n",
      "3          0.498182   2750  \n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. Skill Matching System using SVM",
   "id": "a26f9451934ae9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T01:23:23.307811Z",
     "start_time": "2025-03-13T01:23:22.778179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare data for SVM - we'll predict if a user would be interested in \"Machine Learning\" as an example\n",
    "target_skill = \"Machine Learning\"\n",
    "\n",
    "# Check if the target skill is in the user's desired skills\n",
    "df['wants_' + target_skill.replace(' ', '_')] = df['desired_skills'].apply(\n",
    "    lambda x: 1 if target_skill in x else 0\n",
    ")\n",
    "\n",
    "# Feature matrix for prediction\n",
    "X_svm = combined_features\n",
    "y_svm = df['wants_' + target_skill.replace(' ', '_')]\n",
    "\n",
    "# Split data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_svm, y_svm, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train SVM model\n",
    "svm_model = SVC(kernel='linear', probability=True, random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = svm_model.predict(X_test)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Not Interested', 'Interested'],\n",
    "            yticklabels=['Not Interested', 'Interested'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title(f'Confusion Matrix for {target_skill} Interest Prediction')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ],
   "id": "8974883ad6602976",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1556\n",
      "           1       1.00      1.00      1.00      1444\n",
      "\n",
      "    accuracy                           1.00      3000\n",
      "   macro avg       1.00      1.00      1.00      3000\n",
      "weighted avg       1.00      1.00      1.00      3000\n",
      "\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7. User Matching for Peer Learning\n",
   "id": "5eda726af5588dfc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T01:23:23.434606Z",
     "start_time": "2025-03-13T01:23:23.420622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a function to find matching users for peer learning\n",
    "def find_learning_partners(user_id, df, n_recommendations=3):\n",
    "    # Get the user's data\n",
    "    user = df[df['user_id'] == user_id].iloc[0]\n",
    "\n",
    "    # Get the user's desired skills\n",
    "    user_desired_skills = [skill.strip() for skill in user['desired_skills'].split(',')]\n",
    "\n",
    "    # Get the user's current skills\n",
    "    user_current_skills = [skill.strip() for skill in user['skills'].split(',')]\n",
    "\n",
    "    # Find potential matches (users who have skills that our user wants to learn)\n",
    "    potential_matches = []\n",
    "\n",
    "    for _, other_user in df[df['user_id'] != user_id].iterrows():\n",
    "        other_skills = [skill.strip() for skill in other_user['skills'].split(',')]\n",
    "        other_desired_skills = [skill.strip() for skill in other_user['desired_skills'].split(',')]\n",
    "\n",
    "        # Calculate skill match score (how many of user's desired skills the other user has)\n",
    "        skill_match_score = sum(1 for skill in user_desired_skills if skill in other_skills)\n",
    "\n",
    "        # Calculate reciprocal match score (how many of other's desired skills the user has)\n",
    "        reciprocal_match_score = sum(1 for skill in other_desired_skills if skill in user_current_skills)\n",
    "\n",
    "        # Calculate total match score - we can weight these differently if needed\n",
    "        total_match_score = skill_match_score + 0.5 * reciprocal_match_score\n",
    "\n",
    "        if skill_match_score > 0:  # Only consider if there's at least one skill match\n",
    "            potential_matches.append({\n",
    "                'user_id': other_user['user_id'],\n",
    "                'skills': other_user['skills'],\n",
    "                'matching_skills': [skill for skill in user_desired_skills if skill in other_skills],\n",
    "                'can_learn_from_you': [skill for skill in other_desired_skills if skill in user_current_skills],\n",
    "                'match_score': total_match_score\n",
    "            })\n",
    "\n",
    "    # Sort by match score (higher is better)\n",
    "    sorted_matches = sorted(potential_matches, key=lambda x: x['match_score'], reverse=True)\n",
    "\n",
    "    # Return top N recommendations\n",
    "    return sorted_matches[:n_recommendations]\n"
   ],
   "id": "e3e818a5e286a8f5",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T01:23:24.261165Z",
     "start_time": "2025-03-13T01:23:23.620227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test the matching function\n",
    "test_user_id = 1\n",
    "matches = find_learning_partners(test_user_id, df, 3)\n",
    "\n",
    "print(f\"\\nLearning Partner Recommendations for User {test_user_id}:\")\n",
    "for i, match in enumerate(matches, 1):\n",
    "    print(f\"\\nMatch {i}:\")\n",
    "    print(f\"User ID: {match['user_id']}\")\n",
    "    print(f\"Skills: {match['skills']}\")\n",
    "    print(f\"Can teach you: {', '.join(match['matching_skills'])}\")\n",
    "    print(f\"Can learn from you: {', '.join(match['can_learn_from_you'])}\")\n",
    "    print(f\"Match score: {match['match_score']}\")"
   ],
   "id": "8205019c0d48aaa3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learning Partner Recommendations for User 1:\n",
      "\n",
      "Match 1:\n",
      "User ID: 3643\n",
      "Skills: CSS, JavaScript, Excel, Java, Machine Learning, Blockchain\n",
      "Can teach you: CSS, Java, Machine Learning, Blockchain\n",
      "Can learn from you: HTML, SQL\n",
      "Match score: 5.0\n",
      "\n",
      "Match 2:\n",
      "User ID: 5313\n",
      "Skills: HTML, CSS, Excel, Java, Node.js, Machine Learning, Data Science\n",
      "Can teach you: CSS, Java, Machine Learning, Data Science\n",
      "Can learn from you: HTML, SQL\n",
      "Match score: 5.0\n",
      "\n",
      "Match 3:\n",
      "User ID: 5674\n",
      "Skills: HTML, CSS, Python, Node.js, Machine Learning, Blockchain, Data Science\n",
      "Can teach you: CSS, Machine Learning, Blockchain, Data Science\n",
      "Can learn from you: HTML, SQL\n",
      "Match score: 5.0\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 8. NLP Chatbot for Skill Guidance",
   "id": "a18d8a11f53f3d1a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T01:23:24.274549Z",
     "start_time": "2025-03-13T01:23:24.267875Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# First, let's create a dataset of common questions and their responses for our skill guidance chatbot\n",
    "skill_conversation_data = [\n",
    "    {\n",
    "        'question': 'What courses should I take to learn Machine Learning?',\n",
    "        'answer': 'To learn Machine Learning, start with Python, Statistics, and Linear Algebra basics, then take courses on ML algorithms, neural networks, and practical ML projects.'\n",
    "    },\n",
    "    {\n",
    "        'question': 'How do I learn Data Science?',\n",
    "        'answer': 'To learn Data Science, I recommend courses in Python or R, statistics, data visualization, machine learning, and big data technologies. Start with the fundamentals and then work on real-world projects.'\n",
    "    },\n",
    "    {\n",
    "        'question': 'What skills do I need for web development?',\n",
    "        'answer': 'For web development, core skills include HTML, CSS, and JavaScript. For frontend, learn frameworks like React, Vue, or Angular. For backend, consider Node.js, Django, or Ruby on Rails, along with database skills.'\n",
    "    },\n",
    "    {\n",
    "        'question': 'How long does it take to learn Python?',\n",
    "        'answer': 'Learning Python basics takes 2-4 weeks, becoming proficient takes 3-6 months, and mastery requires ongoing practice. The timeline depends on your prior programming experience and study consistency.'\n",
    "    },\n",
    "    {\n",
    "        'question': 'Which is better to learn first, Java or Python?',\n",
    "        'answer': 'Python is often recommended for beginners due to its simpler syntax and readability. Java has a steeper learning curve but is valuable for enterprise applications. Choose based on your career goals.'\n",
    "    },\n",
    "    {\n",
    "        'question': 'What should I learn after HTML and CSS?',\n",
    "        'answer': 'After HTML and CSS, learn JavaScript to add interactivity to websites. Then consider a frontend framework like React, Vue, or Angular, and basic backend concepts for full-stack development.'\n",
    "    },\n",
    "    {\n",
    "        'question': 'How to start learning AI?',\n",
    "        'answer': 'Start learning AI with Python programming, statistics, and linear algebra. Then progress to machine learning fundamentals, neural networks, and specialized areas like NLP or computer vision.'\n",
    "    },\n",
    "    {\n",
    "        'question': 'What are the best resources to learn SQL?',\n",
    "        'answer': 'Great SQL learning resources include interactive platforms like SQLZoo and Mode Analytics, courses on Coursera and DataCamp, and practice through real database projects.'\n",
    "    },\n",
    "    {\n",
    "        'question': 'How do I find a study partner?',\n",
    "        'answer': 'You can find a study partner by using our matching system, joining relevant online communities, participating in forums related to your interests, or attending virtual meetups and hackathons.'\n",
    "    },\n",
    "    {\n",
    "        'question': 'What skills are in demand right now?',\n",
    "        'answer': 'Currently in-demand skills include Machine Learning, Data Science, Cloud Computing (AWS/Azure), DevOps, Full-Stack Development, Cybersecurity, and Blockchain development.'\n",
    "    }\n",
    "]\n"
   ],
   "id": "6693abfde284c66f",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preprocess text function (tokenization, removing stopwords, lemmatization)",
   "id": "aa05589b01109542"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T01:23:24.469621Z",
     "start_time": "2025-03-13T01:23:24.439043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nltk.download('punkt_tab')\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Preprocess the conversation data\n",
    "processed_questions = [preprocess_text(item['question']) for item in skill_conversation_data]\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(processed_questions)\n"
   ],
   "id": "80eb7bf7666b375e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\farou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T01:23:24.491605Z",
     "start_time": "2025-03-13T01:23:24.486850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to get the most similar response\n",
    "def get_chatbot_response(user_input, threshold=0.3):\n",
    "    # Preprocess the user input\n",
    "    processed_input = preprocess_text(user_input)\n",
    "\n",
    "    # Vectorize the user input\n",
    "    user_vector = tfidf_vectorizer.transform([processed_input])\n",
    "\n",
    "    # Calculate similarity scores\n",
    "    similarity_scores = cosine_similarity(user_vector, tfidf_matrix)[0]\n",
    "\n",
    "    # Get the index of the most similar question\n",
    "    max_similarity_index = np.argmax(similarity_scores)\n",
    "    max_similarity = similarity_scores[max_similarity_index]\n",
    "\n",
    "    # If similarity is above threshold, return the corresponding answer\n",
    "    if max_similarity >= threshold:\n",
    "        return skill_conversation_data[max_similarity_index]['answer']\n",
    "    else:\n",
    "        # Generate a generic response for questions not in our dataset\n",
    "        return \"I'm not sure about that specific topic. However, I can help you find courses or study partners for various skills. Could you tell me what skills you're interested in learning?\"\n",
    "\n"
   ],
   "id": "ce2f9aeee379a5b6",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Testing the NLP",
   "id": "7f229c538177bf9f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T01:23:24.580955Z",
     "start_time": "2025-03-13T01:23:24.560196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test the chatbot with sample questions\n",
    "sample_questions = [\n",
    "    \"What should I learn to become a data scientist?\",\n",
    "    \"I want to learn web development, what should I study?\",\n",
    "    \"How can I find someone to study programming with?\",\n",
    "    \"What programming language should I learn first?\",\n",
    "    \"I'm interested in artificial intelligence\"\n",
    "]\n",
    "\n",
    "print(\"\\nChatbot Response Examples:\")\n",
    "for question in sample_questions:\n",
    "    print(f\"\\nQ: {question}\")\n",
    "    print(f\"A: {get_chatbot_response(question)}\")\n",
    "\n",
    "## 9. Building a complete skill guidance system\n",
    "\n",
    "def skill_guidance_system(user_id, user_query, df):\n",
    "    \"\"\"\n",
    "    Combined skill guidance system that leverages all components:\n",
    "    1. Chatbot for general skill guidance\n",
    "    2. User matching for peer learning\n",
    "    3. Course recommendations based on desired skills\n",
    "    \"\"\"\n",
    "    response = {}\n",
    "\n",
    "    # Get chatbot response\n",
    "    chatbot_answer = get_chatbot_response(user_query)\n",
    "    response['guidance'] = chatbot_answer\n",
    "\n",
    "    # If user_id is provided, get personalized recommendations\n",
    "    if user_id and user_id in df['user_id'].values:\n",
    "        # Get matching learning partners\n",
    "        learning_partners = find_learning_partners(user_id, df, n_recommendations=2)\n",
    "        response['learning_partners'] = learning_partners\n",
    "\n",
    "        # Get user's desired skills\n",
    "        user_desired_skills = df[df['user_id'] == user_id]['desired_skills'].iloc[0].split(', ')\n",
    "\n",
    "        # Recommend courses based on desired skills (simple frequency-based recommendation)\n",
    "        recommended_courses = []\n",
    "        for skill in user_desired_skills:\n",
    "            skill = skill.strip()\n",
    "            # Find users who have this skill\n",
    "            users_with_skill = df[df['skills'].apply(lambda x: skill in x)]\n",
    "            # Get courses they took\n",
    "            if not users_with_skill.empty:\n",
    "                courses = []\n",
    "                for course_list in users_with_skill['joinedCourses']:\n",
    "                    courses.extend([c.strip() for c in course_list.split(',')])\n",
    "\n",
    "                # Get most common courses\n",
    "                if courses:\n",
    "                    course_counts = pd.Series(courses).value_counts()\n",
    "                    top_courses = course_counts.head(2).index.tolist()\n",
    "                    recommended_courses.append({\n",
    "                        'skill': skill,\n",
    "                        'recommended_courses': top_courses\n",
    "                    })\n",
    "\n",
    "        response['course_recommendations'] = recommended_courses\n",
    "\n",
    "    return response\n"
   ],
   "id": "13e903abc211530d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chatbot Response Examples:\n",
      "\n",
      "Q: What should I learn to become a data scientist?\n",
      "A: To learn Data Science, I recommend courses in Python or R, statistics, data visualization, machine learning, and big data technologies. Start with the fundamentals and then work on real-world projects.\n",
      "\n",
      "Q: I want to learn web development, what should I study?\n",
      "A: For web development, core skills include HTML, CSS, and JavaScript. For frontend, learn frameworks like React, Vue, or Angular. For backend, consider Node.js, Django, or Ruby on Rails, along with database skills.\n",
      "\n",
      "Q: How can I find someone to study programming with?\n",
      "A: You can find a study partner by using our matching system, joining relevant online communities, participating in forums related to your interests, or attending virtual meetups and hackathons.\n",
      "\n",
      "Q: What programming language should I learn first?\n",
      "A: Python is often recommended for beginners due to its simpler syntax and readability. Java has a steeper learning curve but is valuable for enterprise applications. Choose based on your career goals.\n",
      "\n",
      "Q: I'm interested in artificial intelligence\n",
      "A: I'm not sure about that specific topic. However, I can help you find courses or study partners for various skills. Could you tell me what skills you're interested in learning?\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Whole System Testing",
   "id": "ce7603b5847094dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T01:23:25.351038Z",
     "start_time": "2025-03-13T01:23:24.649777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_user_id = 1\n",
    "test_query = \"I want to learn mACHINE LEARNING AND I DONT KNOW WHERE TO START\"\n",
    "\n",
    "guidance_result = skill_guidance_system(test_user_id, test_query, df)\n",
    "\n",
    "print(\"\\nComplete Skill Guidance System Output:\")\n",
    "print(\"\\nChatbot Guidance:\")\n",
    "print(guidance_result['guidance'])\n",
    "\n",
    "print(\"\\nRecommended Learning Partners:\")\n",
    "for partner in guidance_result['learning_partners']:\n",
    "    print(f\"User {partner['user_id']} - Can teach you: {', '.join(partner['matching_skills'])}\")\n",
    "\n",
    "print(\"\\nCourse Recommendations:\")\n",
    "for rec in guidance_result['course_recommendations']:\n",
    "    print(f\"For {rec['skill']}: {', '.join(rec['recommended_courses'])}\")\n"
   ],
   "id": "94210feb5816d00a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Complete Skill Guidance System Output:\n",
      "\n",
      "Chatbot Guidance:\n",
      "Start learning AI with Python programming, statistics, and linear algebra. Then progress to machine learning fundamentals, neural networks, and specialized areas like NLP or computer vision.\n",
      "\n",
      "Recommended Learning Partners:\n",
      "User 3643 - Can teach you: CSS, Java, Machine Learning, Blockchain\n",
      "User 5313 - Can teach you: CSS, Java, Machine Learning, Data Science\n",
      "\n",
      "Course Recommendations:\n",
      "For CSS: Excel, HTML\n",
      "For Java: CSS, Data Science\n",
      "For Machine Learning: AI, Java\n",
      "For Blockchain: Data Science, HTML\n",
      "For Data Science: CSS, AI\n",
      "\n",
      "\n",
      "Conclusion and Next Steps:\n",
      "\n",
      "In this notebook, we've built a comprehensive skill matching and guidance system that includes:\n",
      "\n",
      "1. Data preparation and cleaning\n",
      "2. Exploratory data analysis with visualizations\n",
      "3. Feature engineering for skills and courses\n",
      "4. User clustering with K-Means (KMM)\n",
      "5. Skill interest prediction with SVM\n",
      "6. User matching for peer learning\n",
      "7. NLP-based chatbot for skill guidance\n",
      "8. Integrated skill guidance system\n",
      "\n",
      "For a production system, next steps would include:\n",
      "- Collecting more user data to improve model accuracy\n",
      "- Implementing a more sophisticated NLP model (e.g., using transformers)\n",
      "- Creating a user interface for the chatbot and recommendation system\n",
      "- Adding a feedback mechanism to improve recommendations over time\n",
      "- Implementing privacy measures for user data protection\n",
      "- Scaling the system for larger user bases\n",
      "\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "ADD SVM IMPLM",
   "id": "4c247ca8bbc55ca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T01:24:30.393678Z",
     "start_time": "2025-03-13T01:23:25.427924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "\n",
    "# Prepare data for association rule mining (convert skills to transaction format)\n",
    "def prepare_transactions(df, column_name):\n",
    "    transactions = []\n",
    "    for skills in df[column_name]:\n",
    "        if isinstance(skills, str):\n",
    "            transactions.append([skill.strip() for skill in skills.split(',')])\n",
    "    return transactions\n",
    "\n",
    "\n",
    "# Get transactions from skills and desired skills\n",
    "skill_transactions = prepare_transactions(df, 'skills')\n",
    "desired_skill_transactions = prepare_transactions(df, 'desired_skills')\n",
    "\n",
    "# Convert to binary format for apriori algorithm\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit_transform(skill_transactions)\n",
    "skill_df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# Find frequent itemsets with Apriori algorithm\n",
    "min_support = 0.01  # Minimum support threshold\n",
    "frequent_itemsets = apriori(skill_df, min_support=min_support, use_colnames=True)\n",
    "\n",
    "# Generate association rules\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.3)\n",
    "\n",
    "# Display top rules by lift\n",
    "print(\"\\nTop Association Rules (by lift):\")\n",
    "if not rules.empty:\n",
    "    top_rules = rules.sort_values('lift', ascending=False).head(10)\n",
    "    for idx, rule in top_rules.iterrows():\n",
    "        antecedents = ', '.join(list(rule['antecedents']))\n",
    "        consequents = ', '.join(list(rule['consequents']))\n",
    "        print(\n",
    "            f\"{antecedents} → {consequents} (Support: {rule['support']:.3f}, Confidence: {rule['confidence']:.3f}, Lift: {rule['lift']:.3f})\")\n",
    "else:\n",
    "    print(\"No rules found with the current thresholds\")\n",
    "\n",
    "# Visualize top rules\n",
    "if not rules.empty:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(rules['support'], rules['confidence'], alpha=0.5, s=rules['lift'] * 20)\n",
    "    plt.xlabel('Support')\n",
    "    plt.ylabel('Confidence')\n",
    "    plt.title('Association Rules - Support vs Confidence')\n",
    "\n",
    "    # Annotate top rules\n",
    "    for idx, rule in top_rules.iterrows():\n",
    "        ant = ', '.join(list(rule['antecedents']))\n",
    "        con = ', '.join(list(rule['consequents']))\n",
    "        label = f\"{ant} → {con}\"\n",
    "        plt.annotate(label,\n",
    "                     (rule['support'], rule['confidence']),\n",
    "                     xytext=(7, -5),\n",
    "                     textcoords='offset points',\n",
    "                     fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('association_rules.png')\n",
    "    plt.close()\n",
    "\n",
    "target_skills = [\"Machine Learning\", \"Python\", \"JavaScript\", \"Data Science\", \"AI\"]\n",
    "svm_results = {}\n",
    "\n",
    "for skill in target_skills:\n",
    "    # Create target variable - does the user want this skill?\n",
    "    df['wants_' + skill.replace(' ', '_')] = df['desired_skills'].apply(\n",
    "        lambda x: 1 if skill in x else 0\n",
    "    )\n",
    "\n",
    "    # Feature matrix for prediction\n",
    "    X_svm = combined_features\n",
    "    y_svm = df['wants_' + skill.replace(' ', '_')]\n",
    "\n",
    "    # Split data for training and testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_svm, y_svm, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Train SVM model\n",
    "    svm_model = SVC(kernel='linear', probability=True, random_state=42)\n",
    "    svm_model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate model\n",
    "    y_pred = svm_model.predict(X_test)\n",
    "\n",
    "    # Calculate performance metrics\n",
    "    accuracy = (y_pred == y_test).mean()\n",
    "    precision = sum((y_pred == 1) & (y_test == 1)) / sum(y_pred == 1) if sum(y_pred == 1) > 0 else 0\n",
    "    recall = sum((y_pred == 1) & (y_test == 1)) / sum(y_test == 1) if sum(y_test == 1) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    svm_results[skill] = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'model': svm_model,\n",
    "        'class_report': classification_report(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "# Display SVM performance for each skill\n",
    "print(\"\\nSVM Performance for Skill Prediction:\")\n",
    "for skill, metrics in svm_results.items():\n",
    "    print(f\"\\n{skill}:\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.3f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.3f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.3f}\")\n",
    "    print(f\"F1-Score: {metrics['f1']:.3f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(metrics['class_report'])\n",
    "\n",
    "# Visualize SVM performance comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "skill_names = list(svm_results.keys())\n",
    "\n",
    "# Create a dataframe for easier plotting\n",
    "svm_metrics_df = pd.DataFrame({\n",
    "    skill: [svm_results[skill][metric] for metric in metrics]\n",
    "    for skill in skill_names\n",
    "}, index=metrics)\n",
    "\n",
    "svm_metrics_df.plot(kind='bar', figsize=(12, 6))\n",
    "plt.title('SVM Performance Metrics by Skill')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Metric')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(title='Skill')\n",
    "plt.tight_layout()\n",
    "plt.savefig('svm_performance.png')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "## Hybrid Recommendation System: Combining ADD and SVM\n",
    "\n",
    "def hybrid_skill_recommendation(user_id, df, svm_results, rules):\n",
    "    \"\"\"\n",
    "    Generate skill recommendations using both association rules and SVM predictions\n",
    "    \"\"\"\n",
    "\n",
    "    # Get user's current skills\n",
    "    if user_id not in df['user_id'].values:\n",
    "        return {\"error\": \"User not found\"}\n",
    "\n",
    "    user_skills = df[df['user_id'] == user_id]['skills'].iloc[0]\n",
    "    if not isinstance(user_skills, str):\n",
    "        user_skills = \"\"\n",
    "    user_skill_list = [skill.strip() for skill in user_skills.split(',')]\n",
    "\n",
    "    # 1. Recommendations from Association Rules\n",
    "    rule_recommendations = set()\n",
    "    if not rules.empty:\n",
    "        for idx, rule in rules.iterrows():\n",
    "            antecedents = set(rule['antecedents'])\n",
    "            if antecedents.issubset(set(user_skill_list)):\n",
    "                consequents = set(rule['consequents'])\n",
    "                # Add consequents that user doesn't already have\n",
    "                for skill in consequents:\n",
    "                    if skill not in user_skill_list:\n",
    "                        rule_recommendations.add(skill)\n",
    "\n",
    "    # 2. Recommendations from SVM models\n",
    "    svm_recommendations = {}\n",
    "    # Create feature vector for this user\n",
    "    user_features = combined_features.loc[df['user_id'] == user_id]\n",
    "\n",
    "    if not user_features.empty:\n",
    "        for skill, data in svm_results.items():\n",
    "            model = data['model']\n",
    "            # Get probability of wanting this skill\n",
    "            skill_prob = model.predict_proba(user_features)[0][1]\n",
    "            if skill not in user_skill_list:  # Only recommend skills the user doesn't have\n",
    "                svm_recommendations[skill] = skill_prob\n",
    "\n",
    "    # Sort SVM recommendations by probability\n",
    "    sorted_svm_recs = sorted(svm_recommendations.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return {\n",
    "        \"association_rule_recommendations\": list(rule_recommendations),\n",
    "        \"svm_recommendations\": [(skill, prob) for skill, prob in sorted_svm_recs]\n",
    "    }\n",
    "\n",
    "\n",
    "# Test the hybrid recommendation system\n",
    "test_user_id = 42  # Change to a user ID in your dataset\n",
    "\n",
    "hybrid_recs = hybrid_skill_recommendation(test_user_id, df, svm_results, rules)\n",
    "\n",
    "print(f\"\\nHybrid Skill Recommendations for User {test_user_id}:\")\n",
    "print(\"\\nRecommendations from Association Rules:\")\n",
    "for skill in hybrid_recs[\"association_rule_recommendations\"][:5]:\n",
    "    print(f\"- {skill}\")\n",
    "\n",
    "print(\"\\nRecommendations from SVM (with probability):\")\n",
    "for skill, prob in hybrid_recs[\"svm_recommendations\"][:5]:\n",
    "    print(f\"- {skill}: {prob:.3f}\")\n",
    "\n",
    "\n",
    "## Algorithm Comparison and Evaluation\n",
    "\n",
    "# Function to evaluate recommendation performance\n",
    "def evaluate_recommendations(df, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Split the data and evaluate both recommendation approaches\n",
    "    \"\"\"\n",
    "    # Split users into train and test sets\n",
    "    user_ids = df['user_id'].unique()\n",
    "    test_users = np.random.RandomState(random_state).choice(\n",
    "        user_ids, size=int(len(user_ids) * test_size), replace=False\n",
    "    )\n",
    "    train_users = np.array([uid for uid in user_ids if uid not in test_users])\n",
    "\n",
    "    # Train data only includes train users\n",
    "    train_df = df[df['user_id'].isin(train_users)]\n",
    "\n",
    "    # Prepare transactions for association rules\n",
    "    train_skill_transactions = prepare_transactions(train_df, 'skills')\n",
    "    te_train = TransactionEncoder()\n",
    "    te_train_ary = te_train.fit_transform(train_skill_transactions)\n",
    "    train_skill_df = pd.DataFrame(te_train_ary, columns=te_train.columns_)\n",
    "\n",
    "    # Find frequent itemsets and rules\n",
    "    train_frequent_itemsets = apriori(train_skill_df, min_support=0.01, use_colnames=True)\n",
    "    train_rules = association_rules(train_frequent_itemsets, metric=\"confidence\", min_threshold=0.3)\n",
    "\n",
    "    # Train SVM models\n",
    "    train_svm_results = {}\n",
    "    for skill in [\"Machine Learning\", \"Python\", \"JavaScript\", \"Data Science\", \"AI\"]:\n",
    "        # Create binary features for skills using the training data\n",
    "        train_skills_binary, _ = create_binary_features(train_df, 'skills')\n",
    "        train_desired_skills_binary, _ = create_binary_features(train_df, 'desired_skills')\n",
    "        train_courses_binary, _ = create_binary_features(train_df, 'joinedCourses')\n",
    "\n",
    "        # Combine features\n",
    "        train_combined = pd.concat([\n",
    "            train_skills_binary,\n",
    "            train_desired_skills_binary,\n",
    "            train_courses_binary,\n",
    "            train_df[['membershipDuration', 'course_count', 'skills_count', 'desired_skills_count',\n",
    "                      'isVerified']].reset_index(drop=True)\n",
    "        ], axis=1)\n",
    "\n",
    "        # Scale numerical features\n",
    "        train_combined[numerical_features] = scaler.transform(train_combined[numerical_features])\n",
    "        train_combined['isVerified'] = train_combined['isVerified'].astype(int)\n",
    "\n",
    "        # Create target variable\n",
    "        train_df['wants_' + skill.replace(' ', '_')] = train_df['desired_skills'].apply(\n",
    "            lambda x: 1 if skill in x else 0\n",
    "        )\n",
    "\n",
    "        # Train SVM\n",
    "        svm_model = SVC(kernel='linear', probability=True, random_state=42)\n",
    "        svm_model.fit(train_combined, train_df['wants_' + skill.replace(' ', '_')])\n",
    "        train_svm_results[skill] = {'model': svm_model}\n",
    "\n",
    "    # Evaluate on test users\n",
    "    precision_add = []\n",
    "    precision_svm = []\n",
    "    recall_add = []\n",
    "    recall_svm = []\n",
    "\n",
    "    for user_id in test_users:\n",
    "        # Get user's desired skills (ground truth)\n",
    "        desired_skills = df[df['user_id'] == user_id]['desired_skills'].iloc[0]\n",
    "        if not isinstance(desired_skills, str) or desired_skills == \"Unknown\":\n",
    "            continue\n",
    "\n",
    "        true_desired = set([skill.strip() for skill in desired_skills.split(',')])\n",
    "\n",
    "        # Get recommendations using both methods\n",
    "        hybrid_recs = hybrid_skill_recommendation(user_id, df, train_svm_results, train_rules)\n",
    "\n",
    "        # ADD recommendations\n",
    "        add_recs = set(hybrid_recs[\"association_rule_recommendations\"])\n",
    "\n",
    "        # SVM recommendations (top 5)\n",
    "        svm_recs = set([skill for skill, _ in hybrid_recs[\"svm_recommendations\"][:5]])\n",
    "\n",
    "        # Calculate precision and recall if we have recommendations and desired skills\n",
    "        if true_desired and add_recs:\n",
    "            add_correct = len(add_recs.intersection(true_desired))\n",
    "            p_add = add_correct / len(add_recs) if len(add_recs) > 0 else 0\n",
    "            r_add = add_correct / len(true_desired) if len(true_desired) > 0 else 0\n",
    "            precision_add.append(p_add)\n",
    "            recall_add.append(r_add)\n",
    "\n",
    "        if true_desired and svm_recs:\n",
    "            svm_correct = len(svm_recs.intersection(true_desired))\n",
    "            p_svm = svm_correct / len(svm_recs) if len(svm_recs) > 0 else 0\n",
    "            r_svm = svm_correct / len(true_desired) if len(true_desired) > 0 else 0\n",
    "            precision_svm.append(p_svm)\n",
    "            recall_svm.append(r_svm)\n",
    "\n",
    "    # Calculate average precision and recall\n",
    "    avg_precision_add = np.mean(precision_add) if precision_add else 0\n",
    "    avg_precision_svm = np.mean(precision_svm) if precision_svm else 0\n",
    "    avg_recall_add = np.mean(recall_add) if recall_add else 0\n",
    "    avg_recall_svm = np.mean(recall_svm) if recall_svm else 0\n",
    "\n",
    "    # Calculate F1 scores\n",
    "    f1_add = 2 * avg_precision_add * avg_recall_add / (avg_precision_add + avg_recall_add) if (\n",
    "                                                                                                          avg_precision_add + avg_recall_add) > 0 else 0\n",
    "    f1_svm = 2 * avg_precision_svm * avg_recall_svm / (avg_precision_svm + avg_recall_svm) if (\n",
    "                                                                                                          avg_precision_svm + avg_recall_svm) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'ADD': {\n",
    "            'precision': avg_precision_add,\n",
    "            'recall': avg_recall_add,\n",
    "            'f1': f1_add\n",
    "        },\n",
    "        'SVM': {\n",
    "            'precision': avg_precision_svm,\n",
    "            'recall': avg_recall_svm,\n",
    "            'f1': f1_svm\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "# Run evaluation\n",
    "evaluation_results = evaluate_recommendations(df)\n",
    "\n",
    "print(\"\\nAlgorithm Evaluation Results:\")\n",
    "print(\"\\nAssociation Rule Discovery (ADD):\")\n",
    "print(f\"Precision: {evaluation_results['ADD']['precision']:.3f}\")\n"
   ],
   "id": "f16bde641054d779",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Association Rules (by lift):\n",
      "SQL, Node.js, Python → CSS (Support: 0.011, Confidence: 0.593, Lift: 1.214)\n",
      "CSS, SQL, Node.js → Python (Support: 0.011, Confidence: 0.359, Lift: 1.206)\n",
      "CSS, AI, HTML → Python (Support: 0.010, Confidence: 0.351, Lift: 1.177)\n",
      "CSS, JavaScript, Node.js → Python (Support: 0.014, Confidence: 0.348, Lift: 1.169)\n",
      "SQL, AI, Excel → HTML (Support: 0.011, Confidence: 0.696, Lift: 1.167)\n",
      "Blockchain, JavaScript → Excel (Support: 0.011, Confidence: 0.591, Lift: 1.163)\n",
      "CSS, Java, Python → SQL (Support: 0.013, Confidence: 0.356, Lift: 1.160)\n",
      "SQL, Java, Python → CSS (Support: 0.013, Confidence: 0.561, Lift: 1.149)\n",
      "SQL, JavaScript, Java → Python (Support: 0.011, Confidence: 0.342, Lift: 1.148)\n",
      "SQL, AI → HTML, Excel (Support: 0.011, Confidence: 0.350, Lift: 1.147)\n",
      "\n",
      "SVM Performance for Skill Prediction:\n",
      "\n",
      "Machine Learning:\n",
      "Accuracy: 1.000\n",
      "Precision: 1.000\n",
      "Recall: 1.000\n",
      "F1-Score: 1.000\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1556\n",
      "           1       1.00      1.00      1.00      1444\n",
      "\n",
      "    accuracy                           1.00      3000\n",
      "   macro avg       1.00      1.00      1.00      3000\n",
      "weighted avg       1.00      1.00      1.00      3000\n",
      "\n",
      "\n",
      "Python:\n",
      "Accuracy: 1.000\n",
      "Precision: 1.000\n",
      "Recall: 1.000\n",
      "F1-Score: 1.000\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1820\n",
      "           1       1.00      1.00      1.00      1180\n",
      "\n",
      "    accuracy                           1.00      3000\n",
      "   macro avg       1.00      1.00      1.00      3000\n",
      "weighted avg       1.00      1.00      1.00      3000\n",
      "\n",
      "\n",
      "JavaScript:\n",
      "Accuracy: 1.000\n",
      "Precision: 1.000\n",
      "Recall: 1.000\n",
      "F1-Score: 1.000\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1951\n",
      "           1       1.00      1.00      1.00      1049\n",
      "\n",
      "    accuracy                           1.00      3000\n",
      "   macro avg       1.00      1.00      1.00      3000\n",
      "weighted avg       1.00      1.00      1.00      3000\n",
      "\n",
      "\n",
      "Data Science:\n",
      "Accuracy: 1.000\n",
      "Precision: 1.000\n",
      "Recall: 1.000\n",
      "F1-Score: 1.000\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1356\n",
      "           1       1.00      1.00      1.00      1644\n",
      "\n",
      "    accuracy                           1.00      3000\n",
      "   macro avg       1.00      1.00      1.00      3000\n",
      "weighted avg       1.00      1.00      1.00      3000\n",
      "\n",
      "\n",
      "AI:\n",
      "Accuracy: 1.000\n",
      "Precision: 1.000\n",
      "Recall: 1.000\n",
      "F1-Score: 1.000\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1218\n",
      "           1       1.00      1.00      1.00      1782\n",
      "\n",
      "    accuracy                           1.00      3000\n",
      "   macro avg       1.00      1.00      1.00      3000\n",
      "weighted avg       1.00      1.00      1.00      3000\n",
      "\n",
      "\n",
      "Hybrid Skill Recommendations for User 42:\n",
      "\n",
      "Recommendations from Association Rules:\n",
      "- CSS\n",
      "- SQL\n",
      "- JavaScript\n",
      "- Python\n",
      "\n",
      "Recommendations from SVM (with probability):\n",
      "- AI: 1.000\n",
      "- Data Science: 1.000\n",
      "- Machine Learning: 1.000\n",
      "- Python: 0.000\n",
      "- JavaScript: 0.000\n",
      "\n",
      "Algorithm Evaluation Results:\n",
      "\n",
      "Association Rule Discovery (ADD):\n",
      "Precision: 0.336\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 62
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
